{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1iCADV855C70OIseS5xOuYGlI-wOFwItE",
      "authorship_tag": "ABX9TyOitIcPgb3gTAA0cZZ0/9p7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanjayyy-22/CodeRxWizard/blob/main/Capstone_medical_prescription_CodeRxWizard.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Working model with Mistral OCR and TinyLlama-1.1B"
      ],
      "metadata": {
        "id": "PyKbSgUlwk7Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies\n",
        "!pip install pyngrok reportlab mistralai transformers datasets accelerate peft trl bitsandbytes torch\n",
        "\n",
        "# Install ngrok\n",
        "!wget https://bin.equinox.io/c/bNyj1mQVY4c/ngrok-v3-stable-linux-amd64.tgz\n",
        "!tar -xvzf ngrok-v3-stable-linux-amd64.tgz\n",
        "!mv ngrok /usr/local/bin/\n",
        "\n",
        "# Authenticate ngrok (replace YOUR_AUTH_TOKEN with your ngrok authtoken)\n",
        "!ngrok authtoken \"NGROK AUTH TOKEN\""
      ],
      "metadata": {
        "id": "pM8FSjQiBfjy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from trl import SFTTrainer\n",
        "\n",
        "# Quantization configuration\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True\n",
        ")\n",
        "\n",
        "# Model ID\n",
        "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "\n",
        "# Load dataset\n",
        "dataset = load_dataset(\"truehealth/medicationqa\")\n",
        "\n",
        "# Reduce dataset size\n",
        "if len(dataset[\"train\"]) > 500:\n",
        "    dataset[\"train\"] = dataset[\"train\"].select(range(500))\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "# Load model with 4-bit quantization\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=quantization_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# Prepare model for k-bit training\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# LoRA configuration\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "# Apply LoRA\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Format dataset for instruction tuning\n",
        "def format_instruction(example):\n",
        "    instruction = f\"\"\"Given the misspelled medication name: '{example['Question']}', identify the correct medication name and explain why it's the correct form.\"\"\"\n",
        "    response = f\"\"\"The correct medication name is '{example['Focus (Drug)']}'. {example['Answer']}\"\"\"\n",
        "    example[\"text\"] = f\"<|user|>\\n{instruction}\\n<|assistant|>\\n{response}</s>\"\n",
        "    return example\n",
        "\n",
        "# Apply formatting\n",
        "formatted_dataset = dataset.map(format_instruction)\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    gradient_checkpointing=True,\n",
        "    optim=\"adamw_torch\",\n",
        "    learning_rate=2e-4,\n",
        "    weight_decay=0.01,\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"epoch\",\n",
        "    eval_strategy=\"no\",\n",
        "    warmup_ratio=0.1,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    max_grad_norm=0.3,\n",
        "    save_total_limit=1\n",
        ")\n",
        "\n",
        "# Formatting function for trainer\n",
        "def formatting_func(example):\n",
        "    return example[\"text\"]\n",
        "\n",
        "# Initialize trainer\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=formatted_dataset[\"train\"],\n",
        "    formatting_func=formatting_func,\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Save the trained adapter\n",
        "model.save_pretrained(\"/content/medical_term_lora_adapter\")\n",
        "tokenizer.save_pretrained(\"/content/medical_term_lora_adapter\")\n",
        "\n",
        "print(\"Model fine-tuning completed!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 887,
          "referenced_widgets": [
            "56b82dc4d526424181d04b7cc450df87",
            "1ab6096446694aadb0d587cbd7282559",
            "42696fb4186e4489a15253156a6823d5",
            "4c64e41047b44bbfbeafbf24637f9296",
            "c19f3b29da5d48819168f131671bb94f",
            "37869d2840444d30adf02e47b3d1eba0",
            "1fca0f83f5a8496b8a5f510a6dd2eefa",
            "d46c7944b34b496f8f35aaedb876078c",
            "acea5521061142b49c85b2c60c57e129",
            "77b3bcb0e19c4435b5d5a47c8e03a797",
            "bffb5e17e6824320a688f1200483f683",
            "444bb40880c34c9fa8a6241338ecbbf1",
            "c5c51baa6ae14a938e8dd78c0e77700a",
            "6f39db451ee6433c9eddc15d146f83a0",
            "edef404ceced4b789915c00df6eb5014",
            "ed876d21a3364099b4ceebc8bec77b4c",
            "b997afde55794c35a9cb48e761df9da9",
            "e3c7dbef07d74b1aa1634e85de994b3c",
            "febb479e352a4fea933880444775c91c",
            "c0c059f4e9e64332bbf665522e86532c",
            "ce693c5bee804a0fb061c1113b44b456",
            "a791a65efccb4a78b0c40408a521420f",
            "8c0946f6396540c9bc5e0c0fed307a9e",
            "bad38c074b5a4618bdbde07dce1b8bad",
            "1c59d88ef69842b7948d3cea8a195092",
            "df5e1838d1ae4538b9aa414fbcdb3165",
            "3ddfd5edbda14c4ea78edac3b185cd7e",
            "84cbbdabb47745f8adcf34cfde0fffd2",
            "106ef47527f849019ea6e5b8216cbd12",
            "4180b0992e2842d99f69995588a6a53b",
            "e0bb132d7b4c488ba05e975610bd8fad",
            "e363c5b0e5b14e748a2c8a6722c7941d",
            "23edb836c03f4fb9a68e4e2e47366271",
            "e4df5b155404404b8be672b625804240",
            "3a45bb6cde0f4d08838905462a1e47e9",
            "b7d9b799f834430f8290f70056a76def",
            "8778e47712c94757b0f799dc50cde479",
            "415da83fa0bd400ca7bd72578fd89de3",
            "48cb874eeff14086933d6334ee8e4914",
            "29bf58273a1b468283c07cfa86d9b5bc",
            "b7bebf96af484f0cb592caf9f64627a7",
            "9507a914e6e642f598b24d2553a1f635",
            "2463c89fb0614f03b92919ddb3bcff51",
            "230262a46de64205a23f8a42a2c5917e",
            "38000422e8564b1e945f3a2dd3114680",
            "4e2cf040f23340e2a6ca56d42727c4db",
            "7cdb69358fda43d086a9d2e50bbdf3ae",
            "b9f48fcf05064d5a942e13151cc03f8b",
            "430938dbdc1b49c1a75a7d36a63f8943",
            "2ccbfb88cfa34b3ca92b9f6d7125de58",
            "fd18386aba3744b5aa48392688c7aa38",
            "4059140ec559429d9b2ef5899dc78de1",
            "52ae04062a094e3dac8bcfa8c1edc084",
            "23a60ef68a944b848d9dc70f165e588e",
            "9b1b15f8e2fd43ceb24cf3fc32132a43",
            "205ffd631aec469f808b928d586989fc",
            "2e36393260b3473d8797229be80f7ca4",
            "637f7d9985af412eae22427fb3bebdda",
            "17ec4731ec504e63939968789dabce2c",
            "759a0ef5b069416cba17b684a038adfa",
            "ad9a465c5f894bfe91639ff4af2d38c2",
            "8c3e5aaccf534998ba78b6fdb2063607",
            "b291543168bf414d8b7ba46ed5fc6d33",
            "3b28220f15f84eeeaa71303b66ae953d",
            "e3c60fb06ba94307a5abb8b608d712db",
            "384f7ace8cf04727be1134a2b72e74be"
          ]
        },
        "id": "e_ZfsIo8kq14",
        "outputId": "11b6347a-87cd-470f-9e99-3af04732a4f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "56b82dc4d526424181d04b7cc450df87"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Applying formatting function to train dataset:   0%|          | 0/500 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "444bb40880c34c9fa8a6241338ecbbf1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Converting train dataset to ChatML:   0%|          | 0/500 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8c0946f6396540c9bc5e0c0fed307a9e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Applying chat template to train dataset:   0%|          | 0/500 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e4df5b155404404b8be672b625804240"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Tokenizing train dataset:   0%|          | 0/500 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "38000422e8564b1e945f3a2dd3114680"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Truncating train dataset:   0%|          | 0/500 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "205ffd631aec469f808b928d586989fc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='186' max='186' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [186/186 13:11, Epoch 2/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>2.066500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.588700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>1.264000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.089600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.171800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>1.150100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>1.071800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>1.042400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>1.110300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.995600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>1.138600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>1.068300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>0.953100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>1.021200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.977300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>0.989100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>0.981500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>1.015200</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model fine-tuning completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/app.py\n",
        "from mistralai import Mistral, DocumentURLChunk, ImageURLChunk, TextChunk\n",
        "from pathlib import Path\n",
        "import streamlit as st\n",
        "from reportlab.lib.pagesizes import letter\n",
        "from reportlab.platypus import SimpleDocTemplate, Paragraph, Table, Spacer\n",
        "from reportlab.lib.styles import getSampleStyleSheet\n",
        "import json\n",
        "import base64\n",
        "import time\n",
        "import os\n",
        "import tempfile\n",
        "\n",
        "# Initialize Mistral client\n",
        "client = Mistral(api_key='lID7ercy0lYWrh4cJap9VxDDLQsesQ8b')\n",
        "\n",
        "# Process prescription image to get JSON\n",
        "def process_prescription(image_file):\n",
        "    assert image_file.is_file()\n",
        "    encoded = base64.b64encode(image_file.read_bytes()).decode()\n",
        "    base64_data_url = f\"data:image/jpeg;base64,{encoded}\"\n",
        "\n",
        "    image_response = client.ocr.process(document=ImageURLChunk(image_url=base64_data_url), model=\"mistral-ocr-latest\")\n",
        "    time.sleep(1)\n",
        "\n",
        "    image_ocr_markdown = image_response.pages[0].markdown\n",
        "\n",
        "    chat_response = client.chat.complete(\n",
        "        model=\"pixtral-12b-latest\",\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    ImageURLChunk(image_url=base64_data_url),\n",
        "                    TextChunk(text=f\"This is image's OCR in markdown:\\n<BEGIN_IMAGE_OCR>\\n{image_ocr_markdown}\\n<END_IMAGE_OCR>.\\nConvert this into a sensible structured json response. The output should be strictly be json with no extra commentary\")\n",
        "                ],\n",
        "            },\n",
        "        ],\n",
        "        response_format={\"type\": \"json_object\"},\n",
        "        temperature=0\n",
        "    )\n",
        "\n",
        "    response_dict = json.loads(chat_response.choices[0].message.content)\n",
        "    json_string = json.dumps(response_dict, indent=4)\n",
        "    return json_string\n",
        "\n",
        "# Format JSON data for UI display\n",
        "def display_json_data(data, level=0):\n",
        "    for key, value in data.items():\n",
        "        # Capitalize and format key\n",
        "        formatted_key = ' '.join(word.capitalize() for word in key.split('_'))\n",
        "        st.markdown(f\"<div class='section-box'>\", unsafe_allow_html=True)\n",
        "        st.markdown(f\"### {formatted_key}\")\n",
        "\n",
        "        if isinstance(value, dict):\n",
        "            # Nested dictionary\n",
        "            for sub_key, sub_value in value.items():\n",
        "                formatted_sub_key = ' '.join(word.capitalize() for word in sub_key.split('_'))\n",
        "                if isinstance(sub_value, list):\n",
        "                    st.write(f\"**{formatted_sub_key}**: {', '.join(str(item) for item in sub_value)}\")\n",
        "                else:\n",
        "                    st.write(f\"**{formatted_sub_key}**: {sub_value or 'N/A'}\")\n",
        "        elif isinstance(value, list):\n",
        "            # List (e.g., medications)\n",
        "            st.write(f\"**{formatted_key}**:\")\n",
        "            if value and isinstance(value[0], dict):\n",
        "                # Table for list of dictionaries\n",
        "                headers = list(value[0].keys())\n",
        "                table_data = [[item.get(h, 'N/A') for h in headers] for item in value]\n",
        "                table_data.insert(0, [h.capitalize() for h in headers])\n",
        "                st.table(table_data)\n",
        "            else:\n",
        "                st.write(', '.join(str(item) for item in value))\n",
        "        else:\n",
        "            # Simple key-value\n",
        "            st.write(f\"**{formatted_key}**: {value or 'N/A'}\")\n",
        "        st.markdown(\"</div>\", unsafe_allow_html=True)\n",
        "\n",
        "# Format JSON data for PDF\n",
        "def add_json_to_pdf(data, elements, styles, level=0):\n",
        "    for key, value in data.items():\n",
        "        # Capitalize and format key\n",
        "        formatted_key = ' '.join(word.capitalize() for word in key.split('_'))\n",
        "        elements.append(Paragraph(formatted_key, styles['Heading2']))\n",
        "\n",
        "        if isinstance(value, dict):\n",
        "            # Nested dictionary\n",
        "            for sub_key, sub_value in value.items():\n",
        "                formatted_sub_key = ' '.join(word.capitalize() for word in sub_key.split('_'))\n",
        "                if isinstance(sub_value, list):\n",
        "                    elements.append(Paragraph(f\"{formatted_sub_key}: {', '.join(str(item) for item in sub_value)}\", styles['Normal']))\n",
        "                else:\n",
        "                    elements.append(Paragraph(f\"{formatted_sub_key}: {sub_value or 'N/A'}\", styles['Normal']))\n",
        "        elif isinstance(value, list):\n",
        "            # List (e.g., medications)\n",
        "            elements.append(Paragraph(formatted_key, styles['Heading3']))\n",
        "            if value and isinstance(value[0], dict):\n",
        "                # Table for list of dictionaries\n",
        "                headers = list(value[0].keys())\n",
        "                table_data = [[h.capitalize() for h in headers]]\n",
        "                for item in value:\n",
        "                    table_data.append([item.get(h, 'N/A') for h in headers])\n",
        "                elements.append(Table(table_data))\n",
        "            else:\n",
        "                elements.append(Paragraph(', '.join(str(item) for item in value), styles['Normal']))\n",
        "        else:\n",
        "            # Simple key-value\n",
        "            elements.append(Paragraph(f\"{formatted_key}: {value or 'N/A'}\", styles['Normal']))\n",
        "        elements.append(Spacer(1, 12))\n",
        "\n",
        "# Generate PDF based on JSON content\n",
        "def generate_pdf(json_data, filename):\n",
        "    data_dict = json.loads(json_data)\n",
        "\n",
        "    doc = SimpleDocTemplate(filename, pagesize=letter)\n",
        "    styles = getSampleStyleSheet()\n",
        "    elements = []\n",
        "\n",
        "    # Title\n",
        "    elements.append(Paragraph(\"Medical Report\", styles['Title']))\n",
        "    elements.append(Spacer(1, 12))\n",
        "\n",
        "    # Check if prescription with medications exists\n",
        "    prescription = data_dict.get(\"prescription\", {})\n",
        "    medications = prescription.get(\"medications\", [])\n",
        "\n",
        "    if medications:\n",
        "        # Prescription case: display patient, age, date, medications\n",
        "        elements.append(Paragraph(\"Prescription Details\", styles['Heading2']))\n",
        "        elements.append(Paragraph(f\"Patient: {prescription.get('patient', 'N/A')}\", styles['Normal']))\n",
        "        elements.append(Paragraph(f\"Age: {prescription.get('age', 'N/A')}\", styles['Normal']))\n",
        "        elements.append(Paragraph(f\"Date: {prescription.get('date', data_dict.get('date', 'N/A'))}\", styles['Normal']))\n",
        "        elements.append(Spacer(1, 12))\n",
        "\n",
        "        # Medications table\n",
        "        elements.append(Paragraph(\"Medications\", styles['Heading3']))\n",
        "        med_data = [[\"Name\", \"Dosage\", \"Frequency\", \"Duration\"]]\n",
        "        for med in medications:\n",
        "            med_data.append([\n",
        "                med.get(\"name\", \"N/A\"),\n",
        "                med.get(\"dosage\", \"N/A\"),\n",
        "                med.get(\"frequency\", \"N/A\"),\n",
        "                med.get(\"duration\", \"N/A\")\n",
        "            ])\n",
        "        elements.append(Table(med_data))\n",
        "    else:\n",
        "        # Non-prescription case: display all JSON data dynamically\n",
        "        add_json_to_pdf(data_dict, elements, styles)\n",
        "\n",
        "    # Build PDF\n",
        "    doc.build(elements)\n",
        "\n",
        "# Streamlit UI\n",
        "st.title(\"Medical Report Processor\")\n",
        "st.write(\"Upload a medical report image (JPEG/PNG) to extract details and generate a PDF.\")\n",
        "\n",
        "# Add CSS for styling\n",
        "st.markdown(\"\"\"\n",
        "    <style>\n",
        "    .section-box {\n",
        "        background-color: #f8f9fa;\n",
        "        padding: 15px;\n",
        "        border-radius: 5px;\n",
        "        margin-bottom: 20px;\n",
        "    }\n",
        "    .stButton>button {\n",
        "        background-color: #4CAF50;\n",
        "        color: white;\n",
        "        border-radius: 5px;\n",
        "    }\n",
        "    </style>\n",
        "\"\"\", unsafe_allow_html=True)\n",
        "\n",
        "uploaded_file = st.file_uploader(\"Choose an image...\", type=[\"jpg\", \"jpeg\", \"png\"])\n",
        "\n",
        "if uploaded_file is not None:\n",
        "    # Save uploaded file temporarily\n",
        "    with tempfile.NamedTemporaryFile(delete=False, suffix=\".jpg\") as tmp:\n",
        "        tmp.write(uploaded_file.read())\n",
        "        tmp_path = tmp.name\n",
        "\n",
        "    try:\n",
        "        # Display the uploaded image\n",
        "        st.image(uploaded_file, caption=\"Uploaded Report\", use_column_width=True)\n",
        "\n",
        "        # Process the image\n",
        "        with st.spinner(\"Processing report...\"):\n",
        "            json_string = process_prescription(Path(tmp_path))\n",
        "\n",
        "        # Parse JSON\n",
        "        data_dict = json.loads(json_string)\n",
        "\n",
        "        # Generate PDF\n",
        "        pdf_path = \"/content/report.pdf\"\n",
        "        generate_pdf(json_string, pdf_path)\n",
        "\n",
        "        # Display formatted output\n",
        "        st.subheader(\"Extracted Report Data\")\n",
        "\n",
        "        # Check if prescription with medications exists\n",
        "        prescription = data_dict.get(\"prescription\", {})\n",
        "        medications = prescription.get(\"medications\", [])\n",
        "\n",
        "        if medications:\n",
        "            # Prescription case\n",
        "            st.markdown(\"<div class='section-box'>\", unsafe_allow_html=True)\n",
        "            st.markdown(\"### Prescription Details\")\n",
        "            st.write(f\"**Patient**: {prescription.get('patient', 'N/A')}\")\n",
        "            st.write(f\"**Age**: {prescription.get('age', 'N/A')}\")\n",
        "            st.write(f\"**Date**: {prescription.get('date', data_dict.get('date', 'N/A'))}\")\n",
        "            st.markdown(\"#### Medications\")\n",
        "            med_data = [[\"Name\", \"Dosage\", \"Frequency\", \"Duration\"]]\n",
        "            for med in medications:\n",
        "                med_data.append([\n",
        "                    med.get(\"name\", \"N/A\"),\n",
        "                    med.get(\"dosage\", \"N/A\"),\n",
        "                    med.get(\"frequency\", \"N/A\"),\n",
        "                    med.get(\"duration\", \"N/A\")\n",
        "                ])\n",
        "            st.table(med_data)\n",
        "            st.markdown(\"</div>\", unsafe_allow_html=True)\n",
        "        else:\n",
        "            # Non-prescription case: display all JSON data dynamically\n",
        "            display_json_data(data_dict)\n",
        "\n",
        "        # Provide download button for PDF\n",
        "        with open(pdf_path, \"rb\") as f:\n",
        "            st.download_button(\n",
        "                label=\"Download Report PDF\",\n",
        "                data=f,\n",
        "                file_name=\"report.pdf\",\n",
        "                mime=\"application/pdf\"\n",
        "            )\n",
        "\n",
        "        # Clean up\n",
        "        os.remove(tmp_path)\n",
        "        os.remove(pdf_path)\n",
        "\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error: {str(e)}\")\n",
        "        if os.path.exists(tmp_path):\n",
        "            os.remove(tmp_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4KGIb1Ylk2Vk",
        "outputId": "33f217c6-a3f3-4afe-bf55-b507800e6c8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "# Start Streamlit in the background\n",
        "process = subprocess.Popen([\"streamlit\", \"run\", \"/content/app.py\", \"--server.port\", \"8501\"])\n",
        "\n",
        "# Start ngrok tunnel\n",
        "public_url = ngrok.connect(8501).public_url\n",
        "print(f\"Streamlit app is live at: {public_url}\")"
      ],
      "metadata": {
        "id": "dTCqPRGIs12I"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}